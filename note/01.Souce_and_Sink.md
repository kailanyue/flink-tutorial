

## 创建项目模板

可以通过官方提供的项目模板来创建项目，具体细节见官方文档：

[ 基于 DataStream API 实现欺诈检测](https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/try-flink/datastream_api.html)

```sh
curl https://flink.apache.org/q/quickstart.sh | bash -s 1.12.1

# java项目
mvn archetype:generate \
    -DarchetypeGroupId=org.apache.flink \
    -DarchetypeArtifactId=flink-quickstart-java \
    -DarchetypeVersion=1.12.1 \
    -DgroupId=com.ngt \
    -DartifactId=frauddetection \
    -Dversion=1.0 \
    -Dpackage=com.ngt \
    -DinteractiveMode=false
    
# Windows环境下删除换行符      
mvn archetype:generate  -DarchetypeGroupId=org.apache.flink -DarchetypeArtifactId=flink-quickstart-java -DarchetypeVersion=1.12.0 -DgroupId=com.ngt -DartifactId=flink-java -Dversion=1.0 -Dpackage=com.ngt -DinteractiveMode=false

# scala 项目    
mvn archetype:generate \
    -DarchetypeGroupId=org.apache.flink \
    -DarchetypeArtifactId=flink-quickstart-scala \
    -DarchetypeVersion=1.12.1 \
    -DgroupId=com.ngt \
    -DartifactId=frauddetection \
    -Dversion=1.0 \
    -Dpackage=com.ngt \
    -DinteractiveMode=false

# Windows环境下删除换行符    
mvn archetype:generate -DarchetypeGroupId=org.apache.flink -DarchetypeArtifactId=flink-quickstart-scala -DarchetypeVersion=1.12.0 -DgroupId=com.ngt -DartifactId=flink-scala -Dversion=1.0 -Dpackage=com.ngt -DinteractiveMode=false
```



Flink提供了不同级别的编程抽象，通过调用抽象的数据集调用算子构建DataFlow就可以实现对分布式的数据进行流式计算和离线计算，**DataSet是批处理的抽象数据集，DataStream是流式计算的抽象数据集**，他们的方法都分别为**Source、Transformation、Sink**

- **Source**主要负责数据的读取
- **Transformation**主要负责对数据的转换操作
- **Sink**负责最终计算好的结果数据输出。

## Source
[Java代码](../java/src/main/java/com/ngt/source/)
[Scala代码](../scala/src/main/scala/com/ngt/source/)

### 通过 WebUI 查看 Source 和 Sink 的并行度

```java
Configuration configuration = new Configuration();
configuration.setInteger("rest.port", 8181);
StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(configuration);
```

访问 localhost:8181 即可通过  WebUI 参看运行情况

### 常用 Source 概述

Source容错的具体细节参考官方文档，[Data Source 和 Sink 的容错保证](https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/guarantees.html)

|         Source         | 是否并行 |             是否无限流              |  容错性  | 备注 |
| :--------------------: | :------: | :---------------------------------: | :------: | :--: |
|      SocketSource      |  非并行  |             无限数据流              | 至多一次 |      |
|      fromElements      |  非并行  |             有限数据流              | 精确一次 |      |
|     fromCollection     |  非并行  |             有限数据流              | 精确一次 |      |
| fromParallelCollection |   并行   |             有限数据流              | 精确一次 |      |
|      fromSequence      |   并行   |             有限数据流              | 精确一次 |      |
|       FileSource       |   并行   | PROCESS_ONCE / PROCESS_CONTINUOUSLY | 精确一次 |      |
|      KafkaSource       |   并行   |             无限数据流              | 精确一次 |      |
|     SourceFunction     |  非并行  |             有限数据流              |          |      |
| ParallelSourceFunction |   并行   |             有限数据流              |          |      |

### SocketSource

用途：通常用于测试

特性：非并行无限数据流

```java
// 地址,端口,分隔符("\n"),最大重试间隔(0)
socketTextStream(String hostname, int port, String delimiter, long maxRetry);
DataStreamSource<String> lines = env.socketTextStream("192.168.31.8", 8888);
```

### FileSource

用途：通常用于检测文件系统的变化并读取

特性：并行数据流，可以通过模式选择有限数据量还是无限数据量

- PROCESS_ONCE：Source 只读取文件中的数据一次，读取完成后，程序退出
- PROCESS_CONTINUOUSLY：Source 会一直监听指定的文件，需要指定检测该文件是否发生变化的时间间隔

```java
/*
流的输入格式 
文件路径 file:///some/local/file" or "hdfs://host:port/file/path
监视类型 PROCESS_ONCE or PROCESS_CONTINUOUSLY
时间间隔 毫秒
*/
DataStreamSource<String> lines = 
    env.readFile(new TextInputFormat(null), path,FileProcessingMode.PROCESS_CONTINUOUSLY, 2000);
```
注意事项：PROCESS_CONTINUOUSLY模式，文件的内容发生变化后，**会将以前的内容和新的内容全部都读取出来，进而造成数据重复读取**

### CollectionSource

用途：通常用于测试

特性：有限数据量，可以是并行或非并行

```java
DataStreamSource<String> elements = env.fromElements("flink", "hadoop", "spark");

DataStreamSource<String> collection = env.fromCollection(Arrays.asList("flink", "hadoop", "spark"));

DataStreamSource<Long> parallelCollection =
    env.fromParallelCollection(new NumberSequenceIterator(1L, 100L),Long.class).setParallelism(4);
// 用于替换已经被标记为过时的 generateSequence 
DataStreamSource<Long> fromSequence = env.fromSequence(1, 100).setParallelism(4);
```

### KafkaSource

[Apache Kafka 连接器](https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/kafka.html)

用途：生产中最常使用的Source，具体细节查看官方文档

特性：并行无限数据量

需要引入Maven依赖

```xml
<dependency>
	<groupId>org.apache.flink</groupId>
	<artifactId>flink-connector-kafka_2.12</artifactId>
	<version>1.12.1</version>
</dependency>
```

```java
// 参数工具
ParameterTool parameterTool = ParameterTool.fromArgs(args);

Properties props = new Properties();
props.setProperty("bootstrap.servers", parameterTool.getRequired("host")); //指定Kafka的Broker地址
props.setProperty("group.id", parameterTool.get("groupId")); //指定组ID
props.setProperty("auto.offset.reset", "earliest"); //如果没有记录偏移量，第一次从最开始消费
props.setProperty("enable.auto.commit", "false");   //kafka的消费者是否自动提交偏移量

FlinkKafkaConsumer<String> kafkaSource = new FlinkKafkaConsumer<>(parameterTool.getRequired("topic"),new SimpleStringSchema(),props);
DataStream<String> lines = env.addSource(kafkaSource);
```



### CustomSource

用途：自定义数据流

特性：有限数据量，可以是非并行或并行的

```java
// 实现 SourceFunction 接口的Source是非并行，有限的数据量
public static class MySource1 implements SourceFunction<String> {
    @Override
    public void run(SourceContext<String> ctx) throws Exception {
        List<String> words = Arrays.asList("a", "b", "c", "d", "e");
        for (String word : words) {
            ctx.collect(word);
        }
    }

    @Override
    public void cancel() { } // WubUI中的 Cancel Job按钮触发该函数
}

// 实现 SourceFunction 接口Source是多并行的，有限的数据量
public static class MySource2 implements ParallelSourceFunction<String> {
    @Override
    public void run(SourceContext<String> ctx) throws Exception {
        List<String> words = Arrays.asList("a", "b", "c", "d", "e");
        for (String word : words) {
            ctx.collect(word);
        }
    }

    @Override
    public void cancel() { } // WubUI中的 Cancel Job按钮触发该函数
}

// 继承Rich，复函数可以获取运行时上下文
public static class MySource extends RichParallelSourceFunction<String> {
    // 1. 调用构造方法
    // 2. 调用 open 方法
    // 3. 调用 run 方法
    // 4. 调用 cancel 方法  WubUI中的 Cancel Job按钮触发该函数
    // 5. 调用 close 方法
}
```



![image-20210125165114818](image/image-20210125165114818.png)

## Sink
[Java代码](../java/src/main/java/com/ngt/sink/)
[Scala代码](../scala/src/main/scala/com/ngt/sink/)

[Data Source 和 Sink 的容错保证](https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/guarantees.html)


|      Sink       |       容错性        |                    备注                    |
| :-------------: | :-----------------: | :----------------------------------------: |
|  SocketSource   |      至少一次       |                                            |
|   File sinks    |      精确一次       |                                            |
|  Elasticsearch  |      至少一次       |                                            |
| Kafka producer  | 至少一次 / 精确一次 | 当使用事务生产者时，保证精确一次 (v 0.11+) |
|      Redis      |      至少一次       |                                            |
| Standard output |                     |                                            |
|    JDBCSink     | 至少一次 / 精确一次 |     精确一次需要 upsert 语句或幂等更新     |

### SocketSink

```java
DataStreamSource<String> lines = env.socketTextStream("192.168.31.8", 8888);
lines.writeToSocket("192.168.31.8", 9999, new SimpleStringSchema());
```



### Filesinks

[ File Sink](https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/file_sink.html)

1. 需要**开启 Checkpointing** 

2. 写入 HDFS 需要在 core-site.xml 文件中配置以下参数，启用 FileStream 

```xml
<property>
    <name>fs.default.name</name>
    <value>hdfs://hadoop102:9000</value>
</property>
```

3. 需要在代码中**设置访问的用户**

```java
System.setProperty("HADOOP_USER_NAME", "ngt"); // ngt 为有 hdfs 文件读写权限的用户 
```

4. 生成滚动文件的条件：每隔固定时间、间隔一段时间没有新文件。文件大小到达预定值

```java
env.enableCheckpointing(5000);

DefaultRollingPolicy<String, String> rollingPolicy = DefaultRollingPolicy.builder()
    .withRolloverInterval(TimeUnit.SECONDS.toMillis(30))       // 30 秒滚动生成一个文件
    .withInactivityInterval(TimeUnit.SECONDS.toMillis(5))      // 最近 5 秒没有收到新的记录生成一个文件
    .withMaxPartSize(1240L * 1024L * 1000L)                    // 当文件达到100M滚动生成一个文件
    .build();

StreamingFileSink<String> sink = StreamingFileSink.forRowFormat(
    new Path("hdfs://192.168.100.102:9000/tmp/Checkpointing"),  //指的文件存储目录
    new SimpleStringEncoder<String>("UTF-8")) //指的文件的编码
    .withRollingPolicy(rollingPolicy)                     //传入文件滚动生成策略
    .build();

lines.addSink(sink);
```



### Kafkaproducer

[Apache Kafka 连接器](https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/kafka.html)

```java
FlinkKafkaProducer<String> kafkaProducer = new FlinkKafkaProducer<>("192.168.100.102:9092",
                                                                    "sinktest", new SimpleStringSchema());
```



### PrintSink

通过继承 RichSinkFunction 实现自定义打印


```java
localhost.addSink(new MyPrintSink()).name("MyPrintSink");
public static class MyPrintSink extends RichSinkFunction<String> {
    int indexOfThisSubtask;

    @Override
    public void open(Configuration parameters) throws Exception {
        indexOfThisSubtask = getRuntimeContext().getIndexOfThisSubtask();
        super.open(parameters);
    }

    @Override
    public void invoke(String value, Context context) throws Exception {
        System.out.println(indexOfThisSubtask + 1 + "> " + value);
    }
}
```

### JDBCSink

1. 添加相关的 Maven 依赖

```xml
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-jdbc_2.12</artifactId>
    <version>1.12.1</version>
</dependency>

<dependency>
    <groupId>mysql</groupId>
    <artifactId>mysql-connector-java</artifactId>
    <version>5.1.48</version>
</dependency>
```

2. 数据库中表必须存在，并且word 必须是主键才能达到更新数据的目的

```sql
CREATE TABLE wordcount(word VARCHAR(20) KEY,COUNT INT);
```

```java
summed.addSink(JdbcSink.sink("insert into wordcount(word,count) values (?,?) on duplicate key update count = ?",
      (ps, t) -> {
          // word 必须是主键才能达到更新数据的目的
          ps.setString(1, t.f0);
          ps.setInt(2, t.f1);
          ps.setInt(3, t.f1);
      },
      new JdbcConnectionOptions.JdbcConnectionOptionsBuilder()
      .withUrl("jdbc:mysql://hadoop102:3306/test")
      .withDriverName("com.mysql.jdbc.Driver")
      .withUsername("root")
      .withPassword("123456")
      .build()));
env.execute();
```

3. 在 Scala 中暂时没有成功使用 JdbcSink，而是使用自定义Sink的方式实现 JDBC 操作

```scala
summed.addSink(MyJdbcSinkFunc())

case class MyJdbcSinkFunc() extends RichSinkFunction[(String, Int)] {
    var conn: Connection = _
    var updateStmt: PreparedStatement = _

    override def open(parameters: Configuration): Unit = {
      conn = DriverManager.getConnection("jdbc:mysql://hadoop102:3306/test", "root", "123456")
      updateStmt = conn.prepareStatement("insert into wordcount(word,count) values (?,?) on duplicate key update count = ?")
    }

    override def invoke(value: (String, Int), context: SinkFunction.Context): Unit = {
      // 查到就更新，没查到就写入
      updateStmt.setString(1, value._1)
      updateStmt.setInt(2, value._2)
      updateStmt.setInt(3, value._2)
      updateStmt.execute()
    }

    override def close(): Unit = {
      updateStmt.close()
      conn.close()
    }
  }
```



### RedisSink

1. 需要引入依赖

```xml
<dependency>
    <groupId>org.apache.bahir</groupId>
    <artifactId>flink-connector-redis_2.11</artifactId>
    <version>1.0</version>
</dependency>

<!-- 测试版依赖 Aliyun 中央仓库没有-->
<dependency>
    <groupId>org.apache.bahir</groupId>
    <artifactId>flink-connector-redis_2.11</artifactId>
    <version>1.1-SNAPSHOT</version>
</dependency>
```

> 注意 jedis 的版本是 2.8 不支持 redis 集群，密码等一系列新特性，如果要使用相关特性需要使用1.1-SNAPSHOT，或者手动编译依赖，[项目地址]([bahir-flink/flink-connector-redis at master · apache/bahir-flink (github.com)](https://github.com/apache/bahir-flink/tree/master/flink-connector-redis))

```java
FlinkJedisPoolConfig conf = new FlinkJedisPoolConfig.Builder().setHost("192.168.31.8").setPort(6379).build();

summed.addSink(new RedisSink<Tuple2<String, Integer>>(conf, new MyRidesSink()));

public static class MyRidesSink implements RedisMapper<Tuple2<String, Integer>> {
    @Override
    public RedisCommandDescription getCommandDescription() {
        return new RedisCommandDescription(RedisCommand.HSET, "wordconut");
    }

    @Override
    public String getKeyFromData(Tuple2<String, Integer> data) {
        return data.f0;
    }

    @Override
    public String getValueFromData(Tuple2<String, Integer> data) {
        return data.f1.toString();
    }
}
```





